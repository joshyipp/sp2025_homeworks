\documentclass[10pt]{article}
\usepackage{amsmath, amssymb}

\begin{document}

\begin{center}
    \LARGE {Problem Set 1 – Supervised Learning} \\[1em]
    \Large{DS542 – DL4DS} \\[0.5em]
    \large Spring, 2025
\end{center}

\vspace{2em}

\noindent\textbf{Note:} Refer to the equations in the \textit{Understanding Deep Learning} textbook to solve the following problems.

\vspace{2em}

\section*{Problem 2.1}

To walk “downhill” on the loss function (equation 2.5), we measure its gradient with respect to the parameters $\phi_0$ and $\phi_1$. Calculate expressions for the slopes $\frac{\partial L}{\partial \phi_0}$ and $\frac{\partial L}{\partial \phi_1}$.

\vspace{20em}

\section*{Problem 2.2}

Show that we can find the minimum of the loss function in closed-form by setting the expression for the derivatives from Problem 2.1 to zero and solving for $\phi_0$ and $\phi_1$.
test
\end{document}
